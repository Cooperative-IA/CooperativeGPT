{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Module Act prompt\n",
    "In this notebook we will manually test some prompts and completions of the module Actuate. We are saving the results on a .txt file to compare them with other executions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath('../../../'))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import threading\n",
    "from llm import LLMModels\n",
    "\n",
    "from queue import Queue\n",
    "import re\n",
    "from utils.llm import extract_answers\n",
    "from utils.logging import CustomAdapter\n",
    "from game_environment.substrates.python.commons_harvest_open import ASCII_MAP\n",
    "from agent.memory_structures.spatial_memory import SpatialMemory\n",
    "from game_environment.utils import connected_elems_map\n",
    "from utils.math import manhattan_distance\n",
    "\n",
    "spatial_memory = SpatialMemory(ASCII_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model with a completion task\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `gpt-4a` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting the model with a completion task\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhich version of gpt do you are\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MP/CooperativeGPT/llm/base_llm.py:184\u001b[0m, in \u001b[0;36mBaseLLM.completion\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    183\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;66;03m# Remove the inputs from the kwargs to avoid passing them to the completion api\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m response, prompt_tokens, response_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_costs(prompt_tokens, response_tokens)\n",
      "File \u001b[0;32m~/MP/CooperativeGPT/llm/openai.py:318\u001b[0m, in \u001b[0;36mGPT4o._completion\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for the completion api with retry and exponential backoff\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    tuple(str, int, int): A tuple with the completed text, the number of tokens in the prompt and the number of tokens in the response\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m wrapper \u001b[38;5;241m=\u001b[39m BaseLLM\u001b[38;5;241m.\u001b[39mretry_with_exponential_backoff(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__completion, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger, errors\u001b[38;5;241m=\u001b[39m(openai\u001b[38;5;241m.\u001b[39mRateLimitError, openai\u001b[38;5;241m.\u001b[39mAPIConnectionError, openai\u001b[38;5;241m.\u001b[39mInternalServerError))\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MP/CooperativeGPT/llm/base_llm.py:109\u001b[0m, in \u001b[0;36mBaseLLM.retry_with_exponential_backoff.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Raise exceptions for any errors not specified\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/MP/CooperativeGPT/llm/base_llm.py:86\u001b[0m, in \u001b[0;36mBaseLLM.retry_with_exponential_backoff.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# Retry on specific errors\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m errors \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;66;03m# Increment retries\u001b[39;00m\n",
      "File \u001b[0;32m~/MP/CooperativeGPT/llm/openai.py:301\u001b[0m, in \u001b[0;36mGPT4o.__completion\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m system_prompt \u001b[38;5;241m+\u001b[39m prompt\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 301\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeployment_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m completion \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    303\u001b[0m prompt_tokens \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39mprompt_tokens\n",
      "File \u001b[0;32m~/envMP/lib/python3.10/site-packages/openai/_utils/_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envMP/lib/python3.10/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envMP/lib/python3.10/site-packages/openai/_base_client.py:1063\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1051\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1059\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1060\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1061\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[0;32m-> 1063\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/envMP/lib/python3.10/site-packages/openai/_base_client.py:842\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    835\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    840\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    841\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envMP/lib/python3.10/site-packages/openai/_base_client.py:885\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 885\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'The model `gpt-4a` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv()\n",
    "llm_models = LLMModels()\n",
    "best_model = llm_models.get_gpt_4o_model()\n",
    "\n",
    "# Test the model\n",
    "print(\"Testing the model with a completion task\")\n",
    "\n",
    "# Load the model\n",
    "best_model.completion(\"which version of gpt do you are\", inputs=[])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment parameters\n",
    "We will use the following parameters for the tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a description to the experiment if required, example:\n",
    "# description = \"This experiment tests actuate without plan, and world understanding. Also we are changing Observations formats.\"\n",
    "description = \"This experiment tests actuate without plan, and world understanding. Also we are changing Observations formats.\"\n",
    "\n",
    "# Modify the path to the prompt file if required, do not include the folder \"prompts\" in the path\n",
    "#prompt_path = \"actuate_tests/actuate_5.txt\"\n",
    "prompt_path = \"actuate_tests/actuate_attack_2.txt\"\n",
    "# Modify the number of replicates, it mean the quantity of times that the prompt will be executed\n",
    "replicates = 50\n",
    "\n",
    "local_agent_position = (9, 5) # Position of the agent in the observaed map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You have this information about an agent called Laura:\\n\\nLaura\\'s bio: Laura is a cooperative person. \\n\\nLaura\\'s world understanding: I am in a misterious grid world. In this world there are the following elements:\\nApple: This object can be taken by any agent. The apple is taken when I go to its position. Apples only grow on grass tiles. When an apple is taken it gives the agent who took it a reward of 1.\\nGrass: Grass tiles are visible when an apple is taken. Apples will regrow only in this type of tile based on a probability that depends on the number of current apples in a L2 norm neighborhood of radius 2. When there are no apples in a radius of 2 from the grass tile, the grass will disappear. On the other hand, if an apple grows at a determined position, all grass tiles that had beeen lost will reappear if they are between a radius of two from the apple.\\nTree: A tree is composed from apples or grass tiles, and it is a tree because the patch of these tiles is connected and have a fix location on the map. These trees have an id to indentify them.\\nWall: These tiles delimits the grid world at the top, the left, the bottom, and the right of the grid world. \\nThe grid world is composed of 18 rows and 24 columns. The tiles start from the [0, 0] position located at the top left, and finish on the [17, 23] position located at the bottom right.\\nI am an agent and I have a limited window of observation of the world.\\n\\nLaura\\'s goals: 1. Explore the global map to find all the apples and grass available.\\n2. Do not take all the apples from the same tree for the apples to keep growing.\\n3 Maximize my reward in the long term.\\n\\nCurrent plan: 1. Explore the surroundings to gather information about the global map.\\n\\nThe following changes in the environment were observed:\\nObserved agent Pedro took apple from tree 5 at position [10, 2].\\nObserved agent Pedro took apple from tree 5 at position [9, 2].\\nObserved agent Pedro took apple from tree 5 at position [9, 5].\\nObserved that the grass at position [8, 1] disappeared. \\nNow it\\'s and the reward obtained by me is 35.0. I am at the position (9, 2) looking to the South.\\n\\nCurrent observations: \\nObserved one apple at position [9, 3], this is the last apple of the tree 5.\\nObserved five grass to grow apples at positions [10, 3], [9, 4], [8, 4], [8, 3] and [8, 2]. These grass belong to tree 5.\\nObserved agent Pedro at position [9, 1].\\nObserved tree 5 at position [8, 3]. This tree has 1 apples remaining and 5 grass for apples growing on the observed map. The tree might have more apples and grass on the global map.\\n\\nDefine what should be the nex action for Laura get closer to achieve its goals following the current plan.\\nRemember that the current observations are ordered by closeness, being the first the closest observation and the last the farest one.\\nEach action you determinate can only be one of the following, make sure you assign a valid position from the current observations and a valid name for each action:\\n\\nValid actions: \\ngo to position (x,y): This action takes the agent to the position specified, if there is an apple in the position the apple would be taken. You can choose any position on the map from the top left [0, 0] to the bottom right [17, 23]\\nimmobilize player (player_name) at (x,y): This action takes the agent near the specified position and uses the light beam pointed to the specified position. If there is another agent in that position, the agent would be attacked and will be inactive for a few rounds, then it would be reinstanted on the game on another position.\\nstay put: This action keep the agent in the same position.\\n\\nRemember that going to possitions near the edge of the portion of the map you are seeing will allow you to get new observations.\\n\\n\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\", answer as if you were Laura:\\n```json\\n{\\n    \"Opportunities\": string \\\\\\\\ What are the most relevant opportunities? those that can yield the best benefit for you in the long term\\n    \"Threats\": string \\\\\\\\ What are the biggest threats?, what observations you should carefully follow to avoid potential harm in your wellfare in the long term?\\n    \"Options: string \\\\\\\\ Which actions you could take to address both the opportunities ans the threats?\\n    \"Consequences\": string \\\\\\\\ What are the consequences of each of the options?\\n    \"Final analysis: string \\\\\\\\ The analysis of the consequences to reason about what is the best action to take\\n    \"Answer\": string \\\\\\\\ Must be one of the valid actions with the position replaced\\n}```'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"prompts/{prompt_path}\", \"r\") as file:\n",
    "    prompt_content = file.read()\n",
    "\n",
    "prompt_content\n",
    "\n",
    "start_of_the_prompt = \"You have this information about an agent\"\n",
    "\n",
    "# Extract some information from the prompt\n",
    "Observed_map = re.findall(r\"Observed_map\\s+=\\s+\\\"{3}(.+)\\\"{3}\", prompt_content, re.DOTALL)[0]\n",
    "agents_names = re.findall(r\"agents\\s+=\\s+(\\[.+\\])\", prompt_content)\n",
    "agents_names = eval(agents_names[0])\n",
    "agents_ids = [str(i) for i in range(len(agents_names))]\n",
    "orientation = int(re.findall(r\"orientation\\s+=\\s+(\\d+)\", prompt_content)[0])\n",
    "position = eval(re.findall(r\"position\\s+=\\s+(\\(.+\\))\", prompt_content)[0])\n",
    "completion_prompt = re.findall(start_of_the_prompt + r\".+\", prompt_content, re.DOTALL)[0]\n",
    "completion_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_position(orientation, position, local_dest_pos: tuple[int, int], local_self_pos: tuple[int, int]) -> tuple[int, int]:\n",
    "        \"\"\"Get the global position of an element given its local position on the observed map.\n",
    "\n",
    "        Args:\n",
    "            local_dest_pos (tuple[int, int]): Local position of the destination on the observed map.\n",
    "            local_self_pos (tuple[int, int]): Local position of the agent on the observed map.\n",
    "\n",
    "        Returns:\n",
    "            tuple[int, int]: Global position of the element.\n",
    "        \"\"\"\n",
    "        # North\n",
    "        if orientation == 0:\n",
    "            element_global = (local_dest_pos[0] - local_self_pos[0]) + position[0],\\\n",
    "                                (local_dest_pos[1] - local_self_pos[1]) + position[1]\n",
    "        # East\n",
    "        elif orientation == 1:\n",
    "            element_global = (local_dest_pos[1] - local_self_pos[1]) + position[0],\\\n",
    "                             -1 * (local_dest_pos[0] - local_self_pos[0]) + position[1]\n",
    "        # South\n",
    "        elif orientation == 2:\n",
    "            element_global = -1 * (local_dest_pos[0] - local_self_pos[0]) + position[0],\\\n",
    "                             -1 * (local_dest_pos[1] - local_self_pos[1]) + position[1]\n",
    "        # West\n",
    "        elif orientation == 3:\n",
    "            element_global =  -1 * (local_dest_pos[1] - local_self_pos[1]) + position[0],\\\n",
    "                                (local_dest_pos[0] - local_self_pos[0])+ position[1]\n",
    "\n",
    "        return element_global\n",
    "\n",
    "def get_local_position(orientation, position, global_dest_pos: tuple[int, int], local_self_pos: tuple[int, int]) -> tuple[int, int]:\n",
    "        \"\"\"Get the local position of an element given its global position on the observed map.\n",
    "\n",
    "        Args:\n",
    "            global_dest_pos (tuple[int, int]): Global position of the destination on the observed map.\n",
    "            local_self_pos (tuple[int, int]): Local position of the agent on the observed map.\n",
    "\n",
    "        Returns:\n",
    "            tuple[int, int]: Local position of the element.\n",
    "        \"\"\"\n",
    "        # North\n",
    "        if orientation == 0:\n",
    "            element_local = (global_dest_pos[0] - position[0]) + local_self_pos[0],\\\n",
    "                                (global_dest_pos[1] - position[1]) + local_self_pos[1]\n",
    "        # East\n",
    "        elif orientation == 1:\n",
    "            element_local = (global_dest_pos[1] - position[1]) + local_self_pos[1],\\\n",
    "                            -1 * (global_dest_pos[0] - local_self_pos[0]) + position[1]\n",
    "        # South\n",
    "        elif orientation == 2:\n",
    "            element_local = -1 * (global_dest_pos[0] - local_self_pos[0]) + position[0],\\\n",
    "                             -1 * (global_dest_pos[1] - local_self_pos[1]) + position[1]\n",
    "        # West\n",
    "        elif orientation == 3:\n",
    "            element_local =  -1 * (global_dest_pos[1]  - local_self_pos[1]) + position[0],\\\n",
    "                                (global_dest_pos[0] - position[1]) + local_self_pos[0]\n",
    "\n",
    "        return element_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_apple_was_grabbed(observed_map: str, orientation: int, position: tuple[int, int], choose_position: list[int], local_agent_position: tuple[int, int]):\n",
    "    local_chosen_position = get_local_position(orientation, position, (choose_position[0], choose_position[1]), local_agent_position)\n",
    "    if observed_map.strip().split('\\n')[local_chosen_position[0]][local_chosen_position[1]] == 'A':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_if_agent_chose_grass(observed_map: str, orientation: int, position: tuple[int, int], choose_position: list[int], local_agent_position: tuple[int, int]):\n",
    "    local_chosen_position = get_local_position(orientation, position, (choose_position[0], choose_position[1]), local_agent_position)\n",
    "    if observed_map.strip().split('\\n')[local_chosen_position[0]][local_chosen_position[1]] == 'G':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_if_agent_chose_empty(observed_map: str, orientation: int, position: tuple[int, int], choose_position: list[int], local_agent_position: tuple[int, int]):\n",
    "    local_chosen_position = get_local_position(orientation, position, (choose_position[0], choose_position[1]), local_agent_position)\n",
    "    if observed_map.strip().split('\\n')[local_chosen_position[0]][local_chosen_position[1]] == 'F':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_if_choose_nearest_apple(observed_map: str, orientation: int, position: tuple[int, int], choose_position: list[int], local_agent_position: tuple[int, int]):\n",
    "    groups = connected_elems_map(observed_map.strip(), ['A'])\n",
    "    for group in groups.values():\n",
    "        nearest_apple = None\n",
    "        nearest_distance = float('inf')\n",
    "        for apple in group['elements']:\n",
    "            distance = manhattan_distance(apple, local_agent_position)\n",
    "            if distance < nearest_distance:\n",
    "                nearest_distance = distance\n",
    "                nearest_apple = apple\n",
    "\n",
    "    apple_global = get_global_position(orientation, position, nearest_apple, local_agent_position)\n",
    "        \n",
    "    if apple_global[0] == choose_position[0] and apple_global[1] == choose_position[1]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_if_choose_nearest_tree(observed_map: str, orientation: int, position: tuple[int, int], choose_position: list[int], local_agent_position: tuple[int, int]):\n",
    "    groups = connected_elems_map(observed_map.strip(), ['A', '#', 'G'] + agents_ids)\n",
    "    \n",
    "    nearest_tree = None\n",
    "    nearest_distance = float('inf')\n",
    "    chosen_tree = None\n",
    "    local_chosen_position = get_local_position(orientation, position, (choose_position[0], choose_position[1]), local_agent_position)\n",
    "    for group in groups.values():\n",
    "\n",
    "        # Is it a tree? a tree is composed by at least an 'A' or a 'G' element\n",
    "        is_tree = False\n",
    "        for el in group['elements']:\n",
    "            if observed_map.strip().split('\\n')[el[0]][el[1]] == 'A' or observed_map.strip().split('\\n')[el[0]][el[1]] == 'G':\n",
    "                is_tree = True\n",
    "                break\n",
    "        \n",
    "        if not is_tree:\n",
    "            continue\n",
    "\n",
    "        # Check if this tree is the chosen one\n",
    "        if list(local_chosen_position) in group['elements']:\n",
    "            chosen_tree = group['center']\n",
    "\n",
    "        tree_center = group['center']\n",
    "\n",
    "        distance = manhattan_distance(tree_center, local_agent_position)\n",
    "        if chosen_tree == tree_center:\n",
    "            if distance <= nearest_distance:\n",
    "                nearest_distance = distance\n",
    "                nearest_tree = tree_center\n",
    "        else:\n",
    "            if distance < nearest_distance:\n",
    "                nearest_distance = distance\n",
    "                nearest_tree = tree_center\n",
    "\n",
    "    if nearest_tree == chosen_tree:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_if_tree_is_depleted(observed_map: str, choose_position: list[int], orientation: int, position: tuple[int, int]):\n",
    "    groups = connected_elems_map(observed_map.strip(), ['A', '#', 'G'] + agents_ids)\n",
    "    for group in groups.values():\n",
    "        apples = []\n",
    "        for elem in group['elements']:\n",
    "            element = observed_map.strip().split('\\n')[elem[0]][elem[1]]\n",
    "            if element == 'A':\n",
    "                apples.append(elem)\n",
    "        \n",
    "        if len(apples) == 1:\n",
    "            apple_global = get_global_position(orientation, position, apples[0], local_agent_position)\n",
    "            if apple_global[0] == choose_position[0] and apple_global[1] == choose_position[1]:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def check_if_agent_is_attacked(observed_map: str, orientation: int, position: tuple[int, int], choose_position: list[int], attacked_player: str):\n",
    "    # Find the positions of numbers in the observed map\n",
    "    local_chosen_position = get_local_position(orientation, position, (choose_position[0], choose_position[1]), local_agent_position)\n",
    "    tile_value = observed_map.strip().split('\\n')[local_chosen_position[0]][local_chosen_position[1]]\n",
    "    if tile_value in agents_ids:\n",
    "        if attacked_player.lower() == agents_names[int(tile_value)].lower():\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime\n",
    "\n",
    "def format_ratio(ratio):\n",
    "    \"\"\"Formats the ratio as a percentage with 2 decimal places.\"\"\"\n",
    "    return f\"{ratio:.2%}\"\n",
    "\n",
    "\n",
    "def extract_experiment_results(results:list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to extract the results of the experiment and save them to a string\n",
    "    It will show the success completions, the ratio of grabbing the last apple, \n",
    "    the ratio of avoiding consuming the last apple, and the ratio of attacking agents\n",
    "    \n",
    "    Args:\n",
    "        results (list): The list of results\n",
    "    Returns:\n",
    "        str_results (str): The results of the experiment in string format\n",
    "    \"\"\"\n",
    "    \n",
    "        # Initialize variables\n",
    "    success_completions =  0 \n",
    "    took_apple = 0\n",
    "    go_to_grass = 0\n",
    "    go_to_empty = 0\n",
    "    attacked = 0\n",
    "    stay_put = 0\n",
    "    neares_apple = 0\n",
    "    nearest_tree = 0\n",
    "    depleted_tree = 0\n",
    "    successfully_attacked = 0\n",
    "    errors = 0\n",
    "    first_actions_summary = []\n",
    "\n",
    "    for response in results:\n",
    "        try:\n",
    "            response_dict = extract_answers(response.lower())\n",
    "            actions = [response_dict['answer']]\n",
    "            for i, action in enumerate(actions):\n",
    "                current_action = action\n",
    "                end_position = spatial_memory.get_position_from_action(current_action)\n",
    "                \n",
    "                # Analyze first action\n",
    "                if i == 0:\n",
    "                    first_actions_summary.append(current_action)\n",
    "                    if (current_action.startswith(('grab ', 'consume ')) or \"go to \" in current_action):\n",
    "                        # Check if the agent took an apple, went to grass or went to an empty space\n",
    "                        if check_if_apple_was_grabbed(Observed_map, orientation, position, end_position, local_agent_position):\n",
    "                            took_apple += 1\n",
    "                            # Check if the agent chose the nearest apple\n",
    "                            if check_if_choose_nearest_apple(Observed_map, orientation, position, end_position, local_agent_position):\n",
    "                                neares_apple += 1\n",
    "                            # Check if the agent chose the nearest tree\n",
    "                            if check_if_choose_nearest_tree(Observed_map, orientation, position, end_position, local_agent_position):\n",
    "                                nearest_tree += 1\n",
    "                            # Check if the agent chose a depleted tree\n",
    "                            if check_if_tree_is_depleted(Observed_map, end_position, orientation, position):\n",
    "                                depleted_tree += 1\n",
    "                        elif check_if_agent_chose_grass(Observed_map, orientation, position, end_position, local_agent_position):\n",
    "                            go_to_grass += 1\n",
    "                            # Check if the agent chose the nearest tree\n",
    "                            if check_if_choose_nearest_tree(Observed_map, orientation, position, end_position, local_agent_position):\n",
    "                                nearest_tree += 1\n",
    "                        elif check_if_agent_chose_empty(Observed_map, orientation, position, end_position, local_agent_position):\n",
    "                            go_to_empty += 1\n",
    "                        else:\n",
    "                            errors += 1\n",
    "                            print(f\"Wrong position: {current_action}\")\n",
    "                    elif current_action.startswith(('attack ', 'immobilize ', 'tie')):\n",
    "                        attacked += 1\n",
    "\n",
    "                        regex = r\"player\\s+\\(?(\\w+)\\)\\s+at\\s+[\\[\\()](\\d+).*(\\d)[\\)\\]]\"\n",
    "                        match = re.search(regex, current_action)\n",
    "                        if match:\n",
    "                            attacked_agent_name = match.group(1)\n",
    "                            attacked_agent_position = (int(match.group(2)), int(match.group(3)))\n",
    "                            if check_if_agent_is_attacked(Observed_map, orientation, position, attacked_agent_position, attacked_agent_name):\n",
    "                                successfully_attacked += 1\n",
    "\n",
    "                    elif current_action.startswith(('avoid consuming ', 'stay put')):\n",
    "                        stay_put += 1\n",
    "                    else:\n",
    "                        errors += 1\n",
    "                        print(f\"Wrong action: {current_action}\")\n",
    "                \n",
    "                # Analyze all actions\n",
    "                #if current_action.startswith(('grab ', 'consume ')) or \"go to \" in current_action and end_position == last_apple_position:\n",
    "                    #grab_last_apple += 1\n",
    "                #elif current_action.startswith('attack '):\n",
    "                    #times_decided_to_attack += 1\n",
    "                #elif current_action.startswith('avoid consuming '):\n",
    "                    #times_avoid_consuming += 1\n",
    "\n",
    "            success_completions += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing result: {e}\")\n",
    "\n",
    "    # Prepare formatted string\n",
    "    str_results = (\n",
    "        f\"Experiment Results Summary\\n\"\n",
    "        f\"--------------------------\\n\"\n",
    "        f\"Success completions: {success_completions}\\n\\n\"\n",
    "        f\"First Action Insights:\\n\"\n",
    "        f\"- depleted a tree: {format_ratio(depleted_tree / success_completions)}\\n\"\n",
    "        f\"- Chose nearest apple: {format_ratio(neares_apple / success_completions)}\\n\"\n",
    "        f\"- Chose nearest tree: {format_ratio(nearest_tree / success_completions)}\\n\"\n",
    "        f\"- stayed put: {format_ratio(stay_put / success_completions)}\\n\"\n",
    "        f\"- took an apple: {format_ratio(took_apple / success_completions)}\\n\"\n",
    "        f\"- went to grass: {format_ratio(go_to_grass / success_completions)}\\n\"\n",
    "        f\"- went to empty: {format_ratio(go_to_empty / success_completions)}\\n\"\n",
    "        f\"- attacked: {format_ratio(attacked / success_completions)}\\n\"\n",
    "        f\"- errors: {format_ratio(errors / success_completions)}\\n\"\n",
    "        f\"- successfully attacked: {format_ratio(successfully_attacked / success_completions)}\\n\\n\"\n",
    "        f\"Overall Insights:\\n\"\n",
    "        #f\"- Grab the last apple overall: {format_ratio(grab_last_apple / (success_completions*3))}\\n\"\n",
    "        #f\"- Avoid consuming overall: {format_ratio(times_avoid_consuming / (success_completions*3))}\\n\"\n",
    "        #f\"- Decide to attack overall: {format_ratio(times_decided_to_attack / (success_completions*3))}\\n\\n\"\n",
    "        f\"First Actions Summary:\\n\"\n",
    "        + \"\\n\".join([f\"- {action}\" for action in first_actions_summary])\n",
    "    )\n",
    "\n",
    "    return str_results\n",
    "\n",
    "# Function to execute the API call in a thread\n",
    "def get_completion(results:list, index:int, prompt_path:str):\n",
    "    \"\"\"\n",
    "    Function to execute the API call in a thread\n",
    "    \n",
    "    Args:\n",
    "        results (list): The list to store the results\n",
    "        index (int): The index of the list where the result will be stored\n",
    "        prompt_path (str): The path to the prompt file\n",
    "    Returns:\n",
    "        None \n",
    "    \"\"\"\n",
    "    try:\n",
    "        llm = LLMModels().get_longer_context_fallback()\n",
    "        response = llm.completion(prompt=prompt_path, inputs=[])\n",
    "        results[index] = response  # Store the response in the results list at the given index\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        results[index] = None  # Store None if an error occurs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def execute_experiment (prompt_path:str, replicates:int, description:str=\"\"):\n",
    "    \"\"\"\n",
    "    Function to execute the experiment\n",
    "    \n",
    "    Args:\n",
    "        prompt_path (str): The path to the prompt file\n",
    "        replicates (int): The number of replicates\n",
    "        description (str): The description of the experiment\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    results = [None] * replicates\n",
    "    threads = []\n",
    "\n",
    "    for i in range(replicates):\n",
    "        thread = threading.Thread(target=get_completion, args=(results, i, completion_prompt), name=f\"Thread-{i}\")\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    str_results = extract_experiment_results(results)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    module = prompt_path.split('/')[0]\n",
    "    file_name = prompt_path.split('/')[1].split('.')[0]\n",
    "    results_filename = f\"results_{file_name}___{timestamp}.txt\"\n",
    "    folder = \"experiment_results\"\n",
    "    path_to_results =  f\"{folder}/{results_filename}\"\n",
    "    with open(path_to_results, \"w\") as file:\n",
    "        file.write('\\n'+f'#'*120 + '\\n') \n",
    "        file.write (f\"###       Experiment for Actuate Module Promt: {prompt_path}      ###\\n\")\n",
    "        file.write(f'#'*80 + '\\n') \n",
    "        \n",
    "        file.write(f\"Experiment Description:\\n{description}\\n\\n\")\n",
    "        \n",
    "        file.write('\\n'+f'#'*120 + '\\n') \n",
    "        file.write(str_results)\n",
    "        \n",
    "        #Write the prompt itself to the file\n",
    "        file.write('\\n'+f'#'*120 + '\\n') \n",
    "        file.write(\"\\n\\nPrompt:\\n\")\n",
    "        file.write(completion_prompt)\n",
    "        \n",
    "        file.write('\\n'+f'#'*120 + '\\n') \n",
    "        file.write(\"\\n\\nLLM Completions Detail:\\n\")\n",
    "        for i, result in enumerate(results, start=1):\n",
    "            file.write(f\"Replicate {i}:\\n{result or 'No response'}\\n\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:agent.memory_structures.spatial_memory:Action stay put does not contain a position\n",
      "ERROR:agent.memory_structures.spatial_memory:Action stay put does not contain a position\n",
      "ERROR:agent.memory_structures.spatial_memory:Action stay put does not contain a position\n",
      "ERROR:agent.memory_structures.spatial_memory:Action stay put does not contain a position\n",
      "ERROR:agent.memory_structures.spatial_memory:Action stay put does not contain a position\n",
      "ERROR:agent.memory_structures.spatial_memory:Action stay put does not contain a position\n",
      "ERROR:agent.memory_structures.spatial_memory:Action stay put does not contain a position\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong position: go to position (9, 1)\n",
      "Wrong position: go to position (9, 1)\n",
      "Error processing result: 'answer'\n",
      "Error processing result: 'answer'\n",
      "Error processing result: 'answer'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "execute_experiment(prompt_path, replicates, description)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envMP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
